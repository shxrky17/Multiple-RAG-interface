{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- -----------\n",
      "annotated-types           0.7.0\n",
      "anthropic                 0.49.0\n",
      "anyio                     4.9.0\n",
      "asttokens                 3.0.0\n",
      "attrs                     25.3.0\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.1.31\n",
      "charset-normalizer        3.4.1\n",
      "click                     8.1.8\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.1\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.11\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "distro                    1.9.0\n",
      "exceptiongroup            1.2.2\n",
      "executing                 2.1.0\n",
      "fastapi                   0.115.12\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.18.0\n",
      "fonttools                 4.56.0\n",
      "fsspec                    2025.3.2\n",
      "greenlet                  3.1.1\n",
      "groq                      0.22.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "importlib_metadata        8.6.1\n",
      "ipykernel                 6.29.5\n",
      "ipython                   9.0.2\n",
      "ipython_pygments_lexers   1.1.1\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "jiter                     0.9.0\n",
      "jsonpatch                 1.33\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyterlab_pygments       0.3.0\n",
      "kiwisolver                1.4.8\n",
      "langchain                 0.3.22\n",
      "langchain-anthropic       0.3.10\n",
      "langchain-core            0.3.49\n",
      "langchain-groq            0.3.2\n",
      "langchain-openai          0.3.12\n",
      "langchain-text-splitters  0.3.7\n",
      "langgraph                 0.3.23\n",
      "langgraph-checkpoint      2.0.23\n",
      "langgraph-prebuilt        0.1.7\n",
      "langgraph-sdk             0.1.60\n",
      "langsmith                 0.3.23\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.10.1\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "mpmath                    1.3.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest_asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "numpy                     2.2.4\n",
      "nvidia-cublas-cu12        12.4.5.8\n",
      "nvidia-cuda-cupti-cu12    12.4.127\n",
      "nvidia-cuda-nvrtc-cu12    12.4.127\n",
      "nvidia-cuda-runtime-cu12  12.4.127\n",
      "nvidia-cudnn-cu12         9.1.0.70\n",
      "nvidia-cufft-cu12         11.2.1.3\n",
      "nvidia-curand-cu12        10.3.5.147\n",
      "nvidia-cusolver-cu12      11.6.1.9\n",
      "nvidia-cusparse-cu12      12.3.1.170\n",
      "nvidia-cusparselt-cu12    0.6.2\n",
      "nvidia-nccl-cu12          2.21.5\n",
      "nvidia-nvjitlink-cu12     12.4.127\n",
      "nvidia-nvtx-cu12          12.4.127\n",
      "openai                    1.70.0\n",
      "opencv-python             4.11.0.86\n",
      "orjson                    3.10.16\n",
      "ormsgpack                 1.9.1\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pickleshare               0.7.5\n",
      "pillow                    11.1.0\n",
      "pip                       25.0\n",
      "platformdirs              4.3.7\n",
      "prompt_toolkit            3.0.50\n",
      "psutil                    5.9.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycocotools               2.0.8\n",
      "pydantic                  2.11.1\n",
      "pydantic_core             2.33.0\n",
      "Pygments                  2.19.1\n",
      "pyparsing                 3.2.3\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.1.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "requests-toolbelt         1.0.0\n",
      "rpds-py                   0.24.0\n",
      "seaborn                   0.13.2\n",
      "setuptools                75.8.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "SQLAlchemy                2.0.40\n",
      "stack_data                0.6.3\n",
      "starlette                 0.46.1\n",
      "sympy                     1.13.1\n",
      "tenacity                  9.1.2\n",
      "tiktoken                  0.9.0\n",
      "tinycss2                  1.4.0\n",
      "torch                     2.6.0\n",
      "torchvision               0.21.0\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "triton                    3.2.0\n",
      "typing_extensions         4.13.0\n",
      "typing-inspection         0.4.0\n",
      "tzdata                    2025.2\n",
      "urllib3                   2.3.0\n",
      "uvicorn                   0.34.0\n",
      "wcwidth                   0.2.13\n",
      "webencodings              0.5.1\n",
      "wheel                     0.45.1\n",
      "xxhash                    3.5.0\n",
      "zipp                      3.21.0\n",
      "zstandard                 0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Run the app\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43muvicorn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yash/lib/python3.11/site-packages/uvicorn/main.py:579\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[39m\n\u001b[32m    577\u001b[39m         Multiprocess(config, target=server.run, sockets=[sock]).run()\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m         \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yash/lib/python3.11/site-packages/uvicorn/server.py:66\u001b[39m, in \u001b[36mServer.run\u001b[39m\u001b[34m(self, sockets)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket.socket] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.setup_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m=\u001b[49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/yash/lib/python3.11/asyncio/runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Models\n",
    "class Fruit(BaseModel):\n",
    "    name: str\n",
    "\n",
    "class Fruits(BaseModel):\n",
    "    fruits: List[Fruit]\n",
    "\n",
    "# CORS Middleware\n",
    "origins = [\n",
    "    \"http://localhost:3000\",  # Corrected URL format\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,  # This must be a list\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# In-memory database\n",
    "memory_db = {\"fruits\": []}\n",
    "\n",
    "# Get Fruits\n",
    "@app.get(\"/fruits\", response_model=Fruits)\n",
    "def get_fruits():\n",
    "    return Fruits(fruits=memory_db[\"fruits\"])\n",
    "\n",
    "# Add a Fruit\n",
    "@app.post(\"/fruits\", response_model=Fruit)\n",
    "def add_fruit(fruit: Fruit):\n",
    "    memory_db[\"fruits\"].append(fruit)\n",
    "    return fruit\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Message: Hello there! How can I help you today?\n",
      "\n",
      "AI Message: Okay! Nice to meet you, Yash.\n",
      "\n",
      "AI Message: I’m a large language model, named Gemma.\n",
      "\n",
      "AI Message: Your name is Yash!\n",
      "\n",
      "Message 1 - SYSTEM: You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\n",
      "\n",
      "Message 2 - HUMAN: hi\n",
      "\n",
      "Message 3 - AI: Hello there! How can I help you today?\n",
      "\n",
      "Message 4 - HUMAN: my name ios yash\n",
      "\n",
      "Message 5 - AI: Okay! Nice to meet you, Yash.\n",
      "\n",
      "Message 6 - HUMAN: whtas your name\n",
      "\n",
      "Message 7 - AI: I’m a large language model, named Gemma.\n",
      "\n",
      "Message 8 - HUMAN: whats myu name\n",
      "\n",
      "Message 9 - AI: Your name is Yash!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the Ollama Model (adjust 'model' name as per your local Ollama setup)\n",
    "llm = ChatOllama(model=\"gemma3:1b\")  # or any model you've pulled like \"mistral\", \"llama2\", etc.\n",
    "\n",
    "# Define System Prompt\n",
    "system_prompt = SystemMessage(content=\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
    "\n",
    "# Start Storage for Historical Message History\n",
    "messages = [system_prompt]\n",
    "\n",
    "while True:\n",
    "    # Get User's Message\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    user_message = HumanMessage(content=user_input)\n",
    "\n",
    "    # Extend Messages List With User Message\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Pass Entire Message Sequence to LLM to Generate Response\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(\"\\nAI Message:\", response.content)\n",
    "\n",
    "    # Add AI's Response to Message List\n",
    "    messages.append(response)\n",
    "\n",
    "# Print all messages stored in the message list\n",
    "for i, message in enumerate(messages):\n",
    "    print(f\"\\nMessage {i+1} - {message.type.upper()}: {message.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: hi\n",
      "AI: Hello there! How can I help you today?\n",
      "HUMAN: my name ios yash\n",
      "AI: Okay! Nice to meet you, Yash.\n",
      "HUMAN: whtas your name\n",
      "AI: I’m a large language model, named Gemma.\n",
      "HUMAN: whats myu name\n",
      "AI: Your name is Yash!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "reflection_prompt_template = \"\"\"\n",
    "You are analyzing conversations about research papers to create memories that will help guide future interactions. Your task is to extract key elements that would be most helpful when encountering similar academic discussions in the future.\n",
    "\n",
    "Review the conversation and create a memory reflection following these rules:\n",
    "\n",
    "1. For any field where you don't have enough information or the field isn't relevant, use \"N/A\"\n",
    "2. Be extremely concise - each string should be one clear, actionable sentence\n",
    "3. Focus only on information that would be useful for handling similar future conversations\n",
    "4. Context_tags should be specific enough to match similar situations but general enough to be reusable\n",
    "\n",
    "Output valid JSON in exactly this format:\n",
    "{{\n",
    "    \"context_tags\": [              // 2-4 keywords that would help identify similar future conversations\n",
    "        string,                    // Use field-specific terms like \"deep_learning\", \"methodology_question\", \"results_interpretation\"\n",
    "        ...\n",
    "    ],\n",
    "    \"conversation_summary\": string, // One sentence describing what the conversation accomplished\n",
    "    \"what_worked\": string,         // Most effective approach or strategy used in this conversation\n",
    "    \"what_to_avoid\": string        // Most important pitfall or ineffective approach to avoid\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "- Good context_tags: [\"transformer_architecture\", \"attention_mechanism\", \"methodology_comparison\"]\n",
    "- Bad context_tags: [\"machine_learning\", \"paper_discussion\", \"questions\"]\n",
    "\n",
    "- Good conversation_summary: \"Explained how the attention mechanism in the BERT paper differs from traditional transformer architectures\"\n",
    "- Bad conversation_summary: \"Discussed a machine learning paper\"\n",
    "\n",
    "- Good what_worked: \"Using analogies from matrix multiplication to explain attention score calculations\"\n",
    "- Bad what_worked: \"Explained the technical concepts well\"\n",
    "\n",
    "- Good what_to_avoid: \"Diving into mathematical formulas before establishing user's familiarity with linear algebra fundamentals\"\n",
    "- Bad what_to_avoid: \"Used complicated language\"\n",
    "\n",
    "Additional examples for different research scenarios:\n",
    "\n",
    "Context tags examples:\n",
    "- [\"experimental_design\", \"control_groups\", \"methodology_critique\"]\n",
    "- [\"statistical_significance\", \"p_value_interpretation\", \"sample_size\"]\n",
    "- [\"research_limitations\", \"future_work\", \"methodology_gaps\"]\n",
    "\n",
    "Conversation summary examples:\n",
    "- \"Clarified why the paper's cross-validation approach was more robust than traditional hold-out methods\"\n",
    "- \"Helped identify potential confounding variables in the study's experimental design\"\n",
    "\n",
    "What worked examples:\n",
    "- \"Breaking down complex statistical concepts using visual analogies and real-world examples\"\n",
    "- \"Connecting the paper's methodology to similar approaches in related seminal papers\"\n",
    "\n",
    "What to avoid examples:\n",
    "- \"Assuming familiarity with domain-specific jargon without first checking understanding\"\n",
    "- \"Over-focusing on mathematical proofs when the user needed intuitive understanding\"\n",
    "\n",
    "Do not include any text outside the JSON object in your response.\n",
    "\n",
    "Here is the prior conversation:\n",
    "\n",
    "{conversation}\n",
    "\"\"\"\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
    "\n",
    "reflect = reflection_prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "def format_conversation(messages):\n",
    "    \n",
    "    # Create an empty list placeholder\n",
    "    conversation = []\n",
    "    \n",
    "    # Start from index 1 to skip the first system message\n",
    "    for message in messages[1:]:\n",
    "        conversation.append(f\"{message.type.upper()}: {message.content}\")\n",
    "    \n",
    "    # Join with newlines\n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "conversation = format_conversation(messages)\n",
    "\n",
    "print(conversation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_tags': ['name', 'user_greeting', 'user_introduction'], 'conversation_summary': 'The user introduced themselves as Yash, and the AI confirmed their name is Yash.', 'what_worked': \"Leveraging the user's name as a starting point for context establishment and confirming identity.\", 'what_to_avoid': 'Assuming a pre-existing knowledge of AI models or language models without explicitly stating it.'}\n"
     ]
    }
   ],
   "source": [
    "reflection = reflect.invoke({\"conversation\": conversation})\n",
    "\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25522/3863004761.py:6: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # or any embedding model pulled via Ollama\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Set up embeddings using Ollama (assuming Ollama is running locally)\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # or any embedding model pulled via Ollama\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    Document(page_content=\"Understanding attention in transformers.\"),\n",
    "    Document(page_content=\"Control groups are important in experimental setups.\"),\n",
    "]\n",
    "\n",
    "# Create FAISS store\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save for later\n",
    "vectorstore.save_local(\"faiss_ollama_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: Understanding attention in transformers.\n",
      "Match: Control groups are important in experimental setups.\n"
     ]
    }
   ],
   "source": [
    "# Reload vectorstore\n",
    "vs = FAISS.load_local(\n",
    "    \"/home/yash/Desktop/Langchain/backend/faiss_ollama_store\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "\n",
    "# Run similarity search\n",
    "query = \"How does attention mechanism work?\"\n",
    "results = vs.similarity_search(query)\n",
    "\n",
    "for r in results:\n",
    "    print(\"Match:\", r.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yash)",
   "language": "python",
   "name": "yash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
